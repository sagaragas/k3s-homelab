---
# Weekly cleanup of completed pods/jobs
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-completed-pods
spec:
  schedule: "0 4 * * 0"  # Sunday 4am
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cluster-maintenance
          restartPolicy: OnFailure
          containers:
            - name: kubectl
              image: bitnami/kubectl:latest
              command:
                - /bin/sh
                - -c
                - |
                  echo "Cleaning up completed pods..."
                  kubectl delete pods --all-namespaces --field-selector=status.phase==Succeeded
                  kubectl delete pods --all-namespaces --field-selector=status.phase==Failed --ignore-not-found
                  echo "Cleaning up old replicasets..."
                  kubectl get rs --all-namespaces -o json | \
                    jq -r '.items[] | select(.spec.replicas==0) | "\(.metadata.namespace) \(.metadata.name)"' | \
                    while read ns name; do
                      kubectl delete rs "$name" -n "$ns" --ignore-not-found
                    done
                  echo "Cleanup complete!"
---
# Daily health check - restart unhealthy pods
apiVersion: batch/v1
kind: CronJob
metadata:
  name: health-check-restart
spec:
  schedule: "0 5 * * *"  # Daily 5am
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cluster-maintenance
          restartPolicy: OnFailure
          containers:
            - name: kubectl
              image: bitnami/kubectl:latest
              command:
                - /bin/sh
                - -c
                - |
                  echo "Checking for CrashLoopBackOff pods..."
                  kubectl get pods --all-namespaces | grep CrashLoopBackOff | while read ns name rest; do
                    echo "Deleting CrashLoopBackOff pod: $ns/$name"
                    kubectl delete pod "$name" -n "$ns" --ignore-not-found
                  done
                  echo "Checking for stuck ImagePullBackOff pods..."
                  kubectl get pods --all-namespaces | grep ImagePullBackOff | while read ns name rest; do
                    echo "Deleting ImagePullBackOff pod: $ns/$name"
                    kubectl delete pod "$name" -n "$ns" --ignore-not-found
                  done
                  echo "Health check complete!"
---
# Weekly Flux garbage collection
apiVersion: batch/v1
kind: CronJob
metadata:
  name: flux-gc
spec:
  schedule: "0 3 * * 0"  # Sunday 3am
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cluster-maintenance
          restartPolicy: OnFailure
          containers:
            - name: kubectl
              image: bitnami/kubectl:latest
              command:
                - /bin/sh
                - -c
                - |
                  echo "Cleaning up old Helm secrets..."
                  # Keep only last 3 revisions per release
                  for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
                    kubectl get secrets -n "$ns" -l owner=helm --sort-by=.metadata.creationTimestamp 2>/dev/null | \
                      grep "sh.helm.release" | head -n -3 | awk '{print $1}' | \
                      xargs -r kubectl delete secret -n "$ns" --ignore-not-found
                  done
                  echo "Flux GC complete!"
